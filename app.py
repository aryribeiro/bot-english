import streamlit as st
import os
import requests
import time
import hashlib
import logging
import json
from typing import Dict, List, Tuple, Any, Optional
from dotenv import load_dotenv
import sqlite3
from contextlib import contextmanager

# Configura√ß√£o de logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger("chatbot_app")

# Carrega vari√°veis de ambiente do .env
load_dotenv()

# Configura√ß√µes
MAX_HISTORY_TOKENS = 800  # Reduzido para evitar problemas com contexto muito longo
MAX_INPUT_LENGTH = 512  # Limitar tamanho de entrada para evitar erros 400
CACHE_EXPIRY = 3600  # Cache expira em 1 hora (segundos)
MAX_RETRIES = 3  # N√∫mero m√°ximo de tentativas para chamadas √† API
API_TIMEOUT = 30  # Timeout em segundos para chamadas √† API (aumentado)
MAX_CONVERSATION_TURNS = 10  # N√∫mero m√°ximo de turnos na conversa

# Configura√ß√£o do banco de dados para cache
DB_PATH = "chat_cache.db"

@contextmanager
def get_db_connection():
    """Gerencia conex√µes com o banco de dados."""
    conn = None
    try:
        conn = sqlite3.connect(DB_PATH)
        conn.row_factory = sqlite3.Row
        yield conn
    finally:
        if conn:
            conn.close()

def init_db():
    """Inicializa o banco de dados para o cache."""
    try:
        with get_db_connection() as conn:
            conn.execute('''
            CREATE TABLE IF NOT EXISTS cache (
                query_hash TEXT PRIMARY KEY,
                response TEXT NOT NULL,
                timestamp INTEGER NOT NULL
            )
            ''')
            conn.commit()
            logger.info("Banco de dados para cache inicializado")
    except Exception as e:
        logger.error(f"Erro ao inicializar banco de dados: {e}")
        # Continua sem cache se n√£o conseguir inicializar o banco de dados
        pass

class SecretManager:
    """Gerencia o acesso seguro aos segredos."""
    @staticmethod
    def get_api_key() -> Optional[str]:
        """Obt√©m a chave da API de forma segura."""
        # Em produ√ß√£o, considere usar um servi√ßo de gerenciamento de segredos
        # como AWS Secrets Manager, Google Secret Manager, HashiCorp Vault, etc.
        api_key = os.getenv("HF_API_KEY")
        
        # Valida√ß√£o b√°sica da chave
        if not api_key:
            logger.error("Chave de API n√£o encontrada")
            return None
            
        if len(api_key) < 8:  # Verifica√ß√£o simples
            logger.warning("Chave de API suspeita - muito curta")
            
        return api_key

class TokenManager:
    """Gerencia o tamanho do hist√≥rico de conversas."""
    @staticmethod
    def estimate_tokens(text: str) -> int:
        """Estimativa aproximada de tokens em um texto."""
        # Aproxima√ß√£o simples: cada 4 caracteres ~ 1 token
        return len(text) // 4

    @staticmethod
    def truncate_history(
        past_inputs: List[str], 
        past_responses: List[str], 
        max_tokens: int = MAX_HISTORY_TOKENS
    ) -> Tuple[List[str], List[str]]:
        """
        Trunca o hist√≥rico para n√£o exceder o m√°ximo de tokens.
        Preserva intera√ß√µes mais recentes.
        """
        inputs_copy = past_inputs.copy()
        responses_copy = past_responses.copy()
        
        # Limita o n√∫mero de turnos para evitar contextos muito longos
        if len(inputs_copy) > MAX_CONVERSATION_TURNS:
            start_idx = len(inputs_copy) - MAX_CONVERSATION_TURNS
            inputs_copy = inputs_copy[start_idx:]
            responses_copy = responses_copy[start_idx:]
            logger.info(f"Hist√≥rico limitado a {MAX_CONVERSATION_TURNS} turnos")
            return inputs_copy, responses_copy
        
        # Estimativa de tokens totais
        total_tokens = sum(TokenManager.estimate_tokens(msg) for msg in inputs_copy + responses_copy)
        
        # Se estiver abaixo do limite, n√£o √© necess√°rio truncar
        if total_tokens <= max_tokens:
            return inputs_copy, responses_copy
            
        # Remove intera√ß√µes mais antigas at√© estar abaixo do limite
        while total_tokens > max_tokens and inputs_copy and responses_copy:
            oldest_input = inputs_copy.pop(0)
            oldest_response = responses_copy.pop(0)
            total_tokens -= (TokenManager.estimate_tokens(oldest_input) + 
                            TokenManager.estimate_tokens(oldest_response))
            
        logger.info(f"Hist√≥rico truncado para {len(inputs_copy)} intera√ß√µes ({total_tokens} tokens est.)")
        return inputs_copy, responses_copy

    @staticmethod
    def truncate_text(text: str, max_length: int = MAX_INPUT_LENGTH) -> str:
        """Limita o tamanho de um texto para evitar erros de requisi√ß√£o."""
        if len(text) <= max_length:
            return text
        return text[:max_length]

class APICache:
    """Implementa cache para chamadas √† API."""
    @staticmethod
    def compute_hash(data: str) -> str:
        """Calcula um hash √∫nico para a consulta."""
        return hashlib.md5(data.encode()).hexdigest()
        
    @staticmethod
    def get_cached_response(query: str) -> Optional[str]:
        """Recupera resposta em cache se existir e estiver v√°lida."""
        try:
            query_hash = APICache.compute_hash(query)
            current_time = int(time.time())
            
            with get_db_connection() as conn:
                result = conn.execute(
                    "SELECT response, timestamp FROM cache WHERE query_hash = ?", 
                    (query_hash,)
                ).fetchone()
                
                if result and (current_time - result['timestamp']) < CACHE_EXPIRY:
                    logger.info("Cache hit!")
                    return result['response']
        except Exception as e:
            logger.error(f"Erro ao buscar no cache: {e}")
            # Continua sem usar cache se houver erro
                
        logger.info("Cache miss")
        return None
        
    @staticmethod
    def cache_response(query: str, response: str) -> None:
        """Armazena uma resposta no cache."""
        try:
            query_hash = APICache.compute_hash(query)
            current_time = int(time.time())
            
            with get_db_connection() as conn:
                conn.execute(
                    "INSERT OR REPLACE INTO cache (query_hash, response, timestamp) VALUES (?, ?, ?)",
                    (query_hash, response, current_time)
                )
                conn.commit()
                logger.info("Resposta armazenada em cache")
        except Exception as e:
            logger.error(f"Erro ao armazenar no cache: {e}")
            # Continua sem armazenar no cache se houver erro

class HuggingFaceClient:
    """Cliente para API do Hugging Face."""
    def __init__(self):
        self.api_key = SecretManager.get_api_key()
        # Blenderbot-400M-distill √© mais est√°vel para conversas simples
        self.api_url = "https://api-inference.huggingface.co/models/facebook/blenderbot-400M-distill"
        
        if not self.api_key:
            st.error("‚ùå Chave de API n√£o configurada corretamente")
            st.stop()
    
    def query(self, prompt: str) -> str:
        """
        Envia consulta para a API com tratamento de erros e retentativas.
        """
        # Limita o tamanho do prompt para evitar erro 400
        prompt = TokenManager.truncate_text(prompt)
        
        # Verifica cache primeiro
        cached_response = APICache.get_cached_response(prompt)
        if cached_response:
            return cached_response
            
        headers = {"Authorization": f"Bearer {self.api_key}"}
        
        # Para evitar problemas com a API do BlenderBot, usamos o formato mais simples
        payload = {
            "inputs": prompt,
            "options": {
                "wait_for_model": True  # Evita erros 503 por modelo n√£o carregado
            }
        }
        
        # Implementa√ß√£o de retentativas com backoff exponencial
        for attempt in range(1, MAX_RETRIES + 1):
            try:
                logger.info(f"Tentativa {attempt} de {MAX_RETRIES}")
                
                # Log da requisi√ß√£o para depura√ß√£o (sem a chave de API)
                debug_headers = dict(headers)
                debug_headers["Authorization"] = "Bearer [REDACTED]"
                logger.debug(f"Enviando requisi√ß√£o: {json.dumps(payload)}")
                logger.debug(f"Headers: {debug_headers}")
                
                response = requests.post(
                    self.api_url, 
                    headers=headers, 
                    json=payload,
                    timeout=API_TIMEOUT
                )
                
                # Log da resposta para depura√ß√£o
                logger.debug(f"Status code: {response.status_code}")
                logger.debug(f"Resposta: {response.text[:200]}...")
                
                # Verifica√ß√£o espec√≠fica para erro 400
                if response.status_code == 400:
                    error_msg = "Erro na formata√ß√£o da requisi√ß√£o para a API."
                    logger.error(f"Erro 400: {response.text}")
                    # Tenta uma abordagem diferente no pr√≥ximo retry
                    if attempt < MAX_RETRIES:
                        # Tenta simplificar o prompt para pr√≥xima tentativa
                        words = prompt.split()
                        if len(words) > 100:
                            prompt = " ".join(words[-100:])  # Usa apenas as √∫ltimas 100 palavras
                        continue
                    return "‚ö†Ô∏è Erro na requisi√ß√£o: formato inv√°lido. Tente uma mensagem mais curta."
                
                # Verifica os erros HTTP
                response.raise_for_status()
                
                # Processa a resposta
                data = response.json()
                result = self._extract_response(data)
                
                # Se a resposta for muito curta ou vazia, pode ser um erro
                if not result or len(result) < 2:
                    logger.warning("Resposta muito curta ou vazia")
                    if attempt < MAX_RETRIES:
                        time.sleep(2 ** attempt)
                        continue
                    result = "Desculpe, n√£o consegui gerar uma resposta adequada. Poderia reformular?"
                
                # Armazena em cache
                APICache.cache_response(prompt, result)
                
                return result
                
            except requests.exceptions.HTTPError as e:
                if e.response.status_code == 429:
                    logger.warning("Rate limit excedido. Aguardando antes de tentar novamente.")
                    # Backoff exponencial
                    wait_time = 2 ** attempt
                    time.sleep(wait_time)
                elif e.response.status_code >= 500:
                    logger.error(f"Erro no servidor: {e}")
                    if attempt < MAX_RETRIES:
                        wait_time = 2 ** attempt
                        time.sleep(wait_time)
                    else:
                        return "‚ö†Ô∏è Erro no servidor da API. Tente novamente mais tarde."
                else:
                    logger.error(f"Erro na requisi√ß√£o HTTP: {e}")
                    return f"‚ö†Ô∏è Erro na requisi√ß√£o: {e.response.status_code}"
                    
            except requests.exceptions.Timeout:
                logger.error("Timeout na requisi√ß√£o")
                if attempt < MAX_RETRIES:
                    wait_time = 2 ** attempt
                    time.sleep(wait_time)
                else:
                    return "‚ö†Ô∏è Tempo esgotado ao aguardar resposta da API."
                
            except requests.exceptions.RequestException as e:
                logger.error(f"Erro de conex√£o: {e}")
                return "‚ö†Ô∏è Erro de conex√£o com a API."
                
            except json.JSONDecodeError:
                logger.error("Resposta n√£o √© um JSON v√°lido")
                if attempt < MAX_RETRIES:
                    wait_time = 2 ** attempt
                    time.sleep(wait_time)
                else:
                    return "‚ö†Ô∏è Resposta inv√°lida da API."
                
            except Exception as e:
                logger.error(f"Erro desconhecido: {e}")
                return f"‚ö†Ô∏è Ocorreu um erro: {str(e)}"
        
        return "‚ö†Ô∏è N√£o foi poss√≠vel obter uma resposta ap√≥s v√°rias tentativas."
    
    def _extract_response(self, data: Any) -> str:
        """Extrai texto da resposta da API, lidando com diferentes formatos."""
        try:
            if isinstance(data, dict):
                if "generated_text" in data:
                    return data["generated_text"].strip()
                if "error" in data:
                    logger.error(f"API retornou erro: {data['error']}")
                    return f"‚ö†Ô∏è Erro da API: {data['error']}"
                
            if isinstance(data, list) and data:
                if isinstance(data[0], dict) and "generated_text" in data[0]:
                    return data[0]["generated_text"].strip()
                
            logger.warning(f"Formato de resposta inesperado: {json.dumps(data)[:200]}")
            
            # Tenta extrair qualquer texto dispon√≠vel
            if isinstance(data, dict):
                for key, value in data.items():
                    if isinstance(value, str) and len(value) > 5:
                        return value.strip()
                        
            if isinstance(data, list) and data and isinstance(data[0], dict):
                for key, value in data[0].items():
                    if isinstance(value, str) and len(value) > 5:
                        return value.strip()
        except Exception as e:
            logger.error(f"Erro ao extrair resposta: {e}")
            
        return "‚ö†Ô∏è Formato de resposta inesperado da API."

class ChatSession:
    """Gerencia a sess√£o de chat e o hist√≥rico."""
    def __init__(self):
        self.client = HuggingFaceClient()
        
        # Inicializa o hist√≥rico de conversas se n√£o existir
        if "user_inputs" not in st.session_state:
            st.session_state.user_inputs = []
            st.session_state.bot_responses = []
    
    def add_message(self, user_input: str) -> str:
        """
        Adiciona uma mensagem do usu√°rio e gera uma resposta.
        Retorna a resposta.
        """
        # Verifica se a entrada √© v√°lida
        if not user_input or len(user_input.strip()) == 0:
            return "‚ö†Ô∏è Por favor, digite uma mensagem v√°lida."
            
        # Verifica se a entrada n√£o √© muito longa
        if len(user_input) > MAX_INPUT_LENGTH:
            user_input = user_input[:MAX_INPUT_LENGTH]
            logger.warning(f"Entrada truncada para {MAX_INPUT_LENGTH} caracteres")
        
        # Adiciona entrada do usu√°rio
        st.session_state.user_inputs.append(user_input)
        
        # Prepara o contexto com hist√≥rico truncado
        truncated_inputs, truncated_responses = TokenManager.truncate_history(
            st.session_state.user_inputs[:-1],
            st.session_state.bot_responses
        )
        
        # Para limitar as chances de erros 400, simplificamos o formato do prompt
        # Enviando apenas a √∫ltima mensagem e algumas intera√ß√µes anteriores
        # Isso aumenta a estabilidade da API
        if len(truncated_inputs) >= 3:
            # Usamos apenas as 3 √∫ltimas intera√ß√µes para o contexto
            truncated_inputs = truncated_inputs[-3:]
            truncated_responses = truncated_responses[-3:]
        
        # Monta a conversa como texto simplificado
        user_query = user_input
        
        # Se tivermos hist√≥rico, adicionamos contexto m√≠nimo
        if truncated_inputs and truncated_responses:
            context = truncated_inputs[-1] + " " + truncated_responses[-1]
            # Limita o tamanho do contexto
            if len(context) > 200:
                context = context[-200:]
            user_query = context + " " + user_input
        
        # Gera resposta usando formato mais simples para reduzir erros
        try:
            reply = self.client.query(user_query)
        except Exception as e:
            logger.error(f"Erro ao gerar resposta: {e}")
            reply = "‚ö†Ô∏è Ocorreu um erro ao processar sua mensagem. Tente novamente ou reformule."
        
        # Adiciona resposta ao hist√≥rico
        st.session_state.bot_responses.append(reply)
        
        return reply
        
    def get_history(self) -> List[dict]:
        """Retorna o hist√≥rico de conversas em formato estruturado."""
        history = []
        for i, user_msg in enumerate(st.session_state.user_inputs):
            history.append({"role": "user", "content": user_msg})
            if i < len(st.session_state.bot_responses):
                history.append({"role": "assistant", "content": st.session_state.bot_responses[i]})
        return history

class ChatUI:
    """Gerencia a interface do usu√°rio do chat."""
    def __init__(self):
        st.set_page_config(
            page_title="Chat IA Contextual",
            page_icon="ü§ñ",
            layout="centered",
            initial_sidebar_state="auto"
        )
        
        # Estilos CSS personalizados
        st.markdown("""
        <style>
        .user-message {
            background-color: #e6f7ff;
            border-radius: 15px;
            padding: 10px 15px;
            margin: 5px 0;
            border-bottom-right-radius: 5px;
            text-align: right;
            margin-left: 20%;
        }
        .bot-message {
            background-color: #f0f0f0;
            border-radius: 15px;
            padding: 10px 15px;
            margin: 5px 0;
            border-bottom-left-radius: 5px;
            margin-right: 20%;
        }
        .chat-header {
            text-align: center;
            margin-bottom: 20px;
        }
        .stButton button {
            background-color: #4CAF50;
            color: white;
            border-radius: 5px;
            border: none;
            width: 100%;
        }
        .error-message {
            color: #721c24;
            background-color: #f8d7da;
            border: 1px solid #f5c6cb;
            border-radius: 5px;
            padding: 10px;
            margin: 10px 0;
        }
        </style>
        """, unsafe_allow_html=True)
        
    def render_header(self):
        """Renderiza o cabe√ßalho da p√°gina."""
        st.markdown('<div class="chat-header">', unsafe_allow_html=True)
        st.title("ü§ñ Chat com IA (Hugging Face)")
        st.markdown("Converse com um modelo de chat mais fluido (BlenderBot)")
        st.markdown('</div>', unsafe_allow_html=True)
        
    def render_conversation(self, history: List[dict]):
        """Renderiza a conversa com estilos CSS."""
        for message in history:
            if message["role"] == "user":
                st.markdown(
                    f'<div class="user-message">üë§ {message["content"]}</div>', 
                    unsafe_allow_html=True
                )
            else:
                # Checagem se a resposta cont√©m erro para destacar
                content = message["content"]
                if content.startswith("‚ö†Ô∏è"):
                    st.markdown(
                        f'<div class="error-message">ü§ñ {content}</div>',
                        unsafe_allow_html=True
                    )
                else:
                    st.markdown(
                        f'<div class="bot-message">ü§ñ {content}</div>', 
                        unsafe_allow_html=True
                    )
    
    def render_input_form(self) -> Tuple[str, bool]:
        """Renderiza o formul√°rio de entrada."""
        with st.form("chat_form", clear_on_submit=True):
            user_input = st.text_input(
                "Voc√™:", 
                placeholder="Digite sua mensagem aqui...",
                key="user_input"
            )
            col1, col2 = st.columns([4, 1])
            with col2:
                submitted = st.form_submit_button("Enviar")
                
        return user_input, submitted
        
    def render_sidebar(self):
        """Renderiza a barra lateral com informa√ß√µes e controles."""
        with st.sidebar:
            st.header("Sobre")
            st.info(
                "Este chat usa o modelo BlenderBot da Meta atrav√©s da "
                "API da Hugging Face. As respostas s√£o armazenadas em cache "
                "para melhorar o desempenho."
            )
            
            # Bot√£o para limpar conversa com callback
            if st.button("Limpar Conversa", key="clear_btn"):
                # Limpeza expl√≠cita do hist√≥rico
                if "user_inputs" in st.session_state:
                    st.session_state.user_inputs = []
                if "bot_responses" in st.session_state:
                    st.session_state.bot_responses = []
                # Recarrega a p√°gina com st.rerun()
                st.rerun()
                
            st.subheader("Configura√ß√µes")
            # Ajustando o nome para max_tokens para corresponder ao uso no c√≥digo
            st.slider(
                "Contexto m√°ximo (tokens)",
                min_value=100,
                max_value=1000,
                value=MAX_HISTORY_TOKENS,
                step=100,
                key="max_tokens"
            )
            
            # Adiciona informa√ß√µes de depura√ß√£o se necess√°rio
            if st.checkbox("Mostrar debug info", value=False):
                st.write(f"Vers√£o Streamlit: {st.__version__}")
                st.write(f"Turnos na conversa: {len(st.session_state.get('user_inputs', []))}")
                st.write(f"Tamanho do hist√≥rico: {sum(len(m) for m in st.session_state.get('user_inputs', []) + st.session_state.get('bot_responses', []))} caracteres")

def main():
    """Fun√ß√£o principal da aplica√ß√£o."""
    try:
        # Inicializa o banco de dados
        init_db()
        
        # Inicializa componentes
        ui = ChatUI()
        session = ChatSession()
        
        # Renderiza interface
        ui.render_header()
        ui.render_sidebar()
        
        # Renderiza conversas anteriores
        history = session.get_history()
        ui.render_conversation(history)
        
        # Formul√°rio de entrada
        user_input, submitted = ui.render_input_form()
        
        # Processa a entrada do usu√°rio
        if submitted and user_input:
            with st.spinner("Pensando..."):
                try:
                    # Atualiza o valor do MAX_HISTORY_TOKENS com base no slider
                    global MAX_HISTORY_TOKENS
                    MAX_HISTORY_TOKENS = st.session_state.get("max_tokens", MAX_HISTORY_TOKENS)
                    
                    # Processa a mensagem do usu√°rio
                    session.add_message(user_input)
                    
                    # Recarrega a p√°gina
                    st.rerun()
                except Exception as e:
                    logger.error(f"Erro ao processar mensagem: {e}")
                    st.error(f"Ocorreu um erro ao processar sua mensagem: {str(e)}")
                    
    except Exception as e:
        # Melhor tratamento de exce√ß√µes para evitar falhas silenciosas
        logger.error(f"Erro na execu√ß√£o principal: {e}")
        st.error(f"Ocorreu um erro na aplica√ß√£o: {str(e)}")
        st.info("Detalhes para suporte t√©cnico foram registrados no log.")
        
        # Adiciona bot√£o para reiniciar aplica√ß√£o em caso de erro
        if st.button("Reiniciar Aplica√ß√£o"):
            # Limpa todo o estado da sess√£o para um reset completo
            for key in list(st.session_state.keys()):
                del st.session_state[key]
            st.rerun()

if __name__ == "__main__":
    main()